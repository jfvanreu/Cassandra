{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Overview\n",
    "#### This project is composed of two sections. In part I, we process 30 log files and extract their data into a single data file. This file is then used in part II to feed the log data into different database tables. Those tables are created based on the expected queries.\n",
    "# Part I. ETL Pipeline for Pre-Processing the Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Import Python packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import Python packages \n",
    "import pandas as pd\n",
    "import cassandra\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Creating list of filepaths to process original event csv data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/workspace\n",
      "['/home/workspace/event_data/2018-11-27-events.csv', '/home/workspace/event_data/2018-11-04-events.csv', '/home/workspace/event_data/2018-11-07-events.csv', '/home/workspace/event_data/2018-11-09-events.csv', '/home/workspace/event_data/2018-11-19-events.csv', '/home/workspace/event_data/2018-11-05-events.csv', '/home/workspace/event_data/2018-11-22-events.csv', '/home/workspace/event_data/2018-11-16-events.csv', '/home/workspace/event_data/2018-11-26-events.csv', '/home/workspace/event_data/2018-11-24-events.csv', '/home/workspace/event_data/2018-11-29-events.csv', '/home/workspace/event_data/2018-11-15-events.csv', '/home/workspace/event_data/2018-11-20-events.csv', '/home/workspace/event_data/2018-11-06-events.csv', '/home/workspace/event_data/2018-11-18-events.csv', '/home/workspace/event_data/2018-11-21-events.csv', '/home/workspace/event_data/2018-11-10-events.csv', '/home/workspace/event_data/2018-11-23-events.csv', '/home/workspace/event_data/2018-11-02-events.csv', '/home/workspace/event_data/2018-11-28-events.csv', '/home/workspace/event_data/2018-11-03-events.csv', '/home/workspace/event_data/2018-11-13-events.csv', '/home/workspace/event_data/2018-11-30-events.csv', '/home/workspace/event_data/2018-11-12-events.csv', '/home/workspace/event_data/2018-11-01-events.csv', '/home/workspace/event_data/2018-11-14-events.csv', '/home/workspace/event_data/2018-11-25-events.csv', '/home/workspace/event_data/2018-11-08-events.csv', '/home/workspace/event_data/2018-11-17-events.csv', '/home/workspace/event_data/2018-11-11-events.csv']\n",
      "We collected 30 files\n"
     ]
    }
   ],
   "source": [
    "# checking current working directory\n",
    "print(os.getcwd())\n",
    "\n",
    "# Get current folder and subfolder event data\n",
    "filepath = os.getcwd() + '/event_data'\n",
    "\n",
    "# Create a for loop to create a list of files and collect each filepath\n",
    "for root, dirs, files in os.walk(filepath):\n",
    "# join the file path and roots with the subdirectories using glob\n",
    "    file_path_list = glob.glob(os.path.join(root,'*.csv'))\n",
    "    print(file_path_list)\n",
    "\n",
    "print(\"We collected {0} files\".format(len(file_path_list)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Processing the files listed above to create the data csv file that will be used for Apache Casssandra tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 30 files\n",
      "Processing: /home/workspace/event_data/2018-11-27-events.csv\n",
      "Processing: /home/workspace/event_data/2018-11-04-events.csv\n",
      "Processing: /home/workspace/event_data/2018-11-07-events.csv\n",
      "Processing: /home/workspace/event_data/2018-11-09-events.csv\n",
      "Processing: /home/workspace/event_data/2018-11-19-events.csv\n",
      "Processing: /home/workspace/event_data/2018-11-05-events.csv\n",
      "Processing: /home/workspace/event_data/2018-11-22-events.csv\n",
      "Processing: /home/workspace/event_data/2018-11-16-events.csv\n",
      "Processing: /home/workspace/event_data/2018-11-26-events.csv\n",
      "Processing: /home/workspace/event_data/2018-11-24-events.csv\n",
      "Processing: /home/workspace/event_data/2018-11-29-events.csv\n",
      "Processing: /home/workspace/event_data/2018-11-15-events.csv\n",
      "Processing: /home/workspace/event_data/2018-11-20-events.csv\n",
      "Processing: /home/workspace/event_data/2018-11-06-events.csv\n",
      "Processing: /home/workspace/event_data/2018-11-18-events.csv\n",
      "Processing: /home/workspace/event_data/2018-11-21-events.csv\n",
      "Processing: /home/workspace/event_data/2018-11-10-events.csv\n",
      "Processing: /home/workspace/event_data/2018-11-23-events.csv\n",
      "Processing: /home/workspace/event_data/2018-11-02-events.csv\n",
      "Processing: /home/workspace/event_data/2018-11-28-events.csv\n",
      "Processing: /home/workspace/event_data/2018-11-03-events.csv\n",
      "Processing: /home/workspace/event_data/2018-11-13-events.csv\n",
      "Processing: /home/workspace/event_data/2018-11-30-events.csv\n",
      "Processing: /home/workspace/event_data/2018-11-12-events.csv\n",
      "Processing: /home/workspace/event_data/2018-11-01-events.csv\n",
      "Processing: /home/workspace/event_data/2018-11-14-events.csv\n",
      "Processing: /home/workspace/event_data/2018-11-25-events.csv\n",
      "Processing: /home/workspace/event_data/2018-11-08-events.csv\n",
      "Processing: /home/workspace/event_data/2018-11-17-events.csv\n",
      "Processing: /home/workspace/event_data/2018-11-11-events.csv\n",
      "Unfiltered Number of Rows: 8056\n"
     ]
    }
   ],
   "source": [
    "# initiating an empty list of rows that will be generated from each file\n",
    "full_data_rows_list = [] \n",
    "    \n",
    "# for every filepath in the file path list\n",
    "print(\"Processing {0} files\".format(len(file_path_list)))\n",
    "for f in file_path_list:\n",
    "\n",
    "# reading csv file\n",
    "    print(\"Processing:\", f)\n",
    "    with open(f, 'r', encoding = 'utf8', newline='') as csvfile: \n",
    "        # creating a csv reader object \n",
    "        csvreader = csv.reader(csvfile) \n",
    "        next(csvreader)\n",
    "        \n",
    " # extracting each data row one by one and append it        \n",
    "        for line in csvreader:\n",
    "            #print(line)\n",
    "            full_data_rows_list.append(line) \n",
    "            \n",
    "print(\"Unfiltered Number of Rows:\",len(full_data_rows_list))\n",
    "#print(full_data_rows_list)\n",
    "\n",
    "# creating a smaller event data csv file called event_datafile_new csv that will be used to insert data into the \\\n",
    "# Apache Cassandra tables\n",
    "csv.register_dialect('myDialect', quoting=csv.QUOTE_ALL, skipinitialspace=True)\n",
    "\n",
    "with open('event_datafile_new.csv', 'w', encoding = 'utf8', newline='') as f:\n",
    "    writer = csv.writer(f, dialect='myDialect')\n",
    "    writer.writerow(['artist','firstName','gender','itemInSession','lastName','length',\\\n",
    "                'level','location','sessionId','song','userId'])\n",
    "    for row in full_data_rows_list:\n",
    "        if (row[0] == ''):  #skip rows that don't include info on artist\n",
    "            continue\n",
    "        writer.writerow((row[0], row[2], row[3], row[4], row[5], row[6], row[7], row[8], row[12], row[13], row[16]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final number of rows:  6821\n"
     ]
    }
   ],
   "source": [
    "# Checking the number of rows after filtering out the rows that didn't refer to artists\n",
    "with open('event_datafile_new.csv', 'r', encoding = 'utf8') as f:\n",
    "    print(\"Final number of rows: \", sum(1 for line in f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Part II. Apache Cassandra coding portion of the project. \n",
    "\n",
    "#### The output of Part I was to create a CSV file titled <font color=red>event_datafile_new.csv</font> and located within the Worksapce directory. It includes the data that will feed into our database tables and includes the following columns: \n",
    "\n",
    "- artist \n",
    "- firstName of user\n",
    "- gender of user\n",
    "- item number in session\n",
    "- last name of user\n",
    "- length of the song\n",
    "- level (paid or free song)\n",
    "- location of the user\n",
    "- sessionId\n",
    "- song title\n",
    "- userId\n",
    "\n",
    "The image below is a screenshot of what the denormalized data looks like in the <font color=red>**event_datafile_new.csv**</font> after the code above is run:<br>\n",
    "\n",
    "<img src=\"images/image_event_datafile_new.jpg\">\n",
    "\n",
    "Now that we generated the event_datafile_new.csv file, we can start running our Cassandra commands to feed data into the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Creating a Cluster to our local Cassandra instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "try: \n",
    "    cluster = Cluster(['127.0.0.1'])\n",
    "    session = cluster.connect()\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Creating a simple Cassandra keyspace named jfvanreu. No need to replicate the data for this small application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    session.execute(\"\"\"\n",
    "    CREATE KEYSPACE IF NOT EXISTS jfvanreu \n",
    "    WITH REPLICATION = \n",
    "    { 'class' : 'SimpleStrategy', 'replication_factor' : 1 }\"\"\"\n",
    ")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Creating a session to the keyspace defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    session.set_keyspace('jfvanreu')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Now that we have a session to our new keyspace (database), we can create tables. In Cassandra, tables always get structured based on expected queries. So, here are the three queries that we'll use to create our tables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 1. Give me the artist, song title and song's length in the music app history that was heard during  sessionId = 338, and itemInSession  = 4\n",
    "\n",
    "\n",
    "##### 2. Give me only the following: name of artist, song (sorted by itemInSession) and user (first and last name) for userid = 10, sessionid = 182\n",
    "    \n",
    "\n",
    "##### 3. Give me every user name (first and last) in my music app history who listened to the song 'All Hands Against His Own'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Create table musicapp_history based on Query 1 requirements.\n",
    "#### *Query 1:*  Give me the artist, song title and song's length in the music app history that was heard during sessionId = 338, and itemInSession = 4\n",
    "#### We use *sessionId* and *itemInSession* as the primary key, so we can query the database based on those attributes. They also identify each record uniquely. We also use them as first columns for our database for efficiency purpose since Cassandra will use those columns to distribute the data among and within nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "query = \"CREATE TABLE IF NOT EXISTS musicapp_history\"\n",
    "\n",
    "query = query + \"(sessionId int, itemInSession int, artist_name text, song_title text, song_length float, user_firstname text, user_lastname text, PRIMARY KEY (sessionId, itemInSession))\"\n",
    "try:\n",
    "    session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Inserting data from event_datafile_new.csv file into the newly created table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "file = 'event_datafile_new.csv'\n",
    "\n",
    "with open(file, encoding = 'utf8') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    next(csvreader) # skip header\n",
    "    for line in csvreader:\n",
    "        # insert relevant data from event_datafile_csv into musicapp_history table.\n",
    "        query = \"INSERT INTO musicapp_history (sessionId, itemInSession, artist_name, song_title, song_length, user_firstname, user_lastname)\"\n",
    "        query = query + \"VALUES (%s,%s, %s, %s, %s, %s, %s)\"\n",
    "        #align csv data as expected by insert query.\n",
    "        session.execute(query, (int(line[8]), int(line[3]), line[0], line[9], float(line[5]), line[1], line[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Performing a SELECT using Query 1 to verify that the data was properly inserted into the musicapp_history table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1 results:\n",
      "\n",
      "\tArtist Name: Faithless; Song Title: Music Matters (Mark Knight Dub); Song length: 495.30731201171875\n"
     ]
    }
   ],
   "source": [
    "query = \"select artist_name, song_title, song_length from musicapp_history WHERE sessionId=338 and itemInSession=4\"\n",
    "try:\n",
    "    rows = session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "print(\"Query 1 results:\")\n",
    "for row in rows:\n",
    "    print (\"\\n\\tArtist Name: {0}; Song Title: {1}; Song length: {2}\".format(row.artist_name, row.song_title, row.song_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Great news! Query 1 results are inline with the data included in the event_datafile_new.csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Create table songs_by_user based on Query 2 requirements.\n",
    "#### Query 2: Give me only the following: name of artist, song (sorted by itemInSession) and user (first and last name) for userid = 10, sessionid = 182\n",
    "#### We use *userid* *sessionId* and *itemInSession* as the primary key, so we can query the database based on those attributes. We use *userId* as the partition key and *sessionId* and *itemInSession* as clustering columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "query = \"CREATE TABLE IF NOT EXISTS songs_by_user_session\"\n",
    "query = query + \"(userId int, sessionId int, itemInSession int, user_firstname text, user_lastname text, song text, artist_name text, PRIMARY KEY (userId, sessionId, itemInSession))\"\n",
    "try:\n",
    "    session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Inserting data from event_datafile_new.csv file into the newly created table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "file = 'event_datafile_new.csv'\n",
    "\n",
    "with open(file, encoding = 'utf8') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    next(csvreader) # skip header\n",
    "    for line in csvreader:\n",
    "        # insert relevant data from event_datafile_csv into songs_by_user_session table.\n",
    "        query = \"INSERT INTO songs_by_user_session (userId, sessionId, itemInSession, user_firstName, user_lastName, song, artist_name)\"\n",
    "        query = query + \"VALUES (%s,%s, %s, %s, %s, %s, %s)\"\n",
    "        #align csv data as expected by insert query.\n",
    "        session.execute(query, (int(line[10]), int(line[8]), int(line[3]), line[1], line[4], line[9], line[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Performing a SELECT using Query 2 to verify that the data was properly inserted into the songs_by_user_session table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 2 results:\n",
      "\n",
      "\tArtist Name: Down To The Bone; Song Title: Keep On Keepin' On; User First Name: Sylvie, User Last Name: Cruz\n",
      "\n",
      "\tArtist Name: Three Drives; Song Title: Greece 2000; User First Name: Sylvie, User Last Name: Cruz\n",
      "\n",
      "\tArtist Name: Sebastien Tellier; Song Title: Kilometer; User First Name: Sylvie, User Last Name: Cruz\n",
      "\n",
      "\tArtist Name: Lonnie Gordon; Song Title: Catch You Baby (Steve Pitron & Max Sanna Radio Edit); User First Name: Sylvie, User Last Name: Cruz\n"
     ]
    }
   ],
   "source": [
    "query = \"select artist_name, song, user_firstname, user_lastname from songs_by_user_session WHERE userId=10 and sessionId=182\"\n",
    "try:\n",
    "    rows = session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "print(\"Query 2 results:\")\n",
    "for row in rows:\n",
    "    print (\"\\n\\tArtist Name: {0}; Song Title: {1}; User First Name: {2}, User Last Name: {3}\".format(row.artist_name, row.song, row.user_firstname, row.user_lastname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Great news! Query 2 results are inline with the data included in the event_datafile_new.csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Create table users_by_song based on Query 3 requirements.\n",
    "#### Query 3: Give me every user name (first and last) in my music app history who listened to the song 'All Hands Against His Own'\n",
    "#### We use *song* and *userId* as the primary key, so we can query the database based on those attributes. We use *song* as the partition key and *userId* as clustering column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "query = \"CREATE TABLE IF NOT EXISTS users_by_song\"\n",
    "query = query + \"(song text, userId int, user_firstname text, user_lastname text, PRIMARY KEY (song, userId))\"\n",
    "try:\n",
    "    session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Inserting data from event_datafile_new.csv file into the newly created table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "file = 'event_datafile_new.csv'\n",
    "\n",
    "with open(file, encoding = 'utf8') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    next(csvreader) # skip header\n",
    "    for line in csvreader:\n",
    "        # insert relevant data from event_datafile_csv into users_by_song table.\n",
    "        query = \"INSERT INTO users_by_song (song, userId, user_firstName, user_lastName)\"\n",
    "        query = query + \"VALUES (%s,%s, %s, %s)\"\n",
    "        #align csv data as expected by insert query.\n",
    "        session.execute(query, (line[9], int(line[10]), line[1], line[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Performing a SELECT using Query 3 to verify that the data was properly inserted into the users_by_song table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 3 results:\n",
      "\n",
      "\tUser's name: Jacqueline Lynch\n",
      "\n",
      "\tUser's name: Tegan Levine\n",
      "\n",
      "\tUser's name: Sara Johnson\n"
     ]
    }
   ],
   "source": [
    "query = \"select user_firstname, user_lastname from users_by_song WHERE song='All Hands Against His Own'\"\n",
    "try:\n",
    "    rows = session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "print(\"Query 3 results:\")\n",
    "for row in rows:\n",
    "    print (\"\\n\\tUser\\'s name: {0} {1}\".format(row.user_firstname, row.user_lastname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Great news! Query 3 results are inline with the data included in the event_datafile_new.csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Dropping the tables before closing out the sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "query = \"drop table musicapp_history\"\n",
    "try:\n",
    "    rows = session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "query = \"drop table songs_by_user_session\"\n",
    "try:\n",
    "    rows = session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "query = \"drop table users_by_song\"\n",
    "try:\n",
    "    rows = session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Closing the session and cluster connection¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "session.shutdown()\n",
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### In this project, we created a database (keyspace in the Cassandra world). We transfered logging data from a 30 csv files into a single file and use that file to insert data into our 3 database tables. Those 3 tables were created and labeled by analyzing their respective queries. Test queries display the expected results as verified in the .csv file. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
